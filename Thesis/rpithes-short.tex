%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                       rpithes-short.tex                         %
%         Template for a short thesis all in one file             %
%        (titlepage info below assumes masters degree}            %
%  Just run latex (or pdflatex) on this file to see how it looks  %
%      Be sure to run twice to get correct TOC and citations      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%
%  To produce the abstract title page followed by the abstract,
%  see the template file, "abstitle-mas.tex"
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{thesis}
\usepackage{graphicx}   % if you want to include graphics files
\usepackage{pdflscape}
\usepackage[backend=bibtex]{biblatex}

% Use the first command below if you want captions over 1 line indented.
% A side effect of this is to remove the use of bold for captions. 
% To restore bold, also include the second line below.
%\usepackage[hang]{caption}     % to indent subsequent lines of captions
%\renewcommand{\captionfont}{\bfseries} % only needed with caption package;
                                        %   otherwise bold is default)
                                        
%%%%%%%%%%%%%%%%%%%%  supply titlepage info  %%%%%%%%%%%%%%%%%%%%%
\thesistitle{\bf NSL Spindle\\Map-Reduce at the Edge In a V2V Environment}        
\author{William Rory Kronmiller}        
\degree{Master of Science}
\department{Computer Science} % provide your area of study here; e.g.,
%  "Mechanical Engineering", "Nuclear Engineering", "Physics", etc.
\thadviser{Dr. Stacy Patterson}
%\cothadviser{First co-adviser} %if needed
%\cocothadviser{Second co-adviser} % if needed
%  For a masters project use \projadviser instead of \thadviser, 
%  and \coprojadviser and \cocoprojadviser if needed. 
\submitdate{March 3\\(For Graduation May 2017)}        
%\copyrightyear{1685}  % if date omitted, current year is used. 
%%%%%%%%%%%%%%%%%%%%%   end titlepage info  %%%%%%%%%%%%%%%%%%%%%%
      
\begin{document} 
\titlepage             % Print titlepage   
%\copyrightpage        % optional         
\tableofcontents       % required 
\listoftables          % required if there are tables
\listoffigures         % required if there are figures

\specialhead{ACKNOWLEDGMENT}
The acknowledgment text goes here. Unlike chapter headings, 
this heading is not numbered.
% Personal thing

\specialhead{ABSTRACT}
Write your abstract here. Again, the heading does receive a number.

\chapter{INTRODUCTION}
    Spindle serves as a scalable hybrid vehicle-to-vehicle and vehicle-to-internet architecture
    for efficient near-real-time processing of streaming sensor data from network-connected vehicles.
    According to a 2015 Hitachi white-paper, some contemporary vehicles produce as much
    as 25 gigabytes of data every hour with prototype vehicles fitted with "cameras and additional
    sensors" producing up to 250 gigabytes per hour \cite{hitachi}. At the same time, work is underway
    to develop and test a variety of communications systems for connecting vehicles to one another
    and to road infrastructure and the internet \cite{connectivitypaper}. Spindle addresses the problem
    of managing the vast quantities of available vehicle data by applying the map/reduce \cite{mapreduce}
    paradigm to logical clusters of interconnected vehicles. In particular, Spindle exploits edge-computation
    by exchanging data over vehicle-to-vehicle communications systems in order to reduce the amount of data
    that must be sent over the internet to the cloud. Spindle is designed to support the use case
    of a developer or analyst writing a streaming map/reduce program that can then be deployed to the cloud
    and distributed to clusters of vehicles; vehicles in each cluster can apply the map operation to their
    incoming data streams and send the map outputs to a single leader (Clusterhead); the Clusterhead then
    applies the reduce operation to a buffer of incoming mapper outputs and sends the result of the reduce
    operation to the cloud, which forwards the data to the user's client program where the data can be
    displayed or further analyzed.

    Our contribution is to demonstrate the bandwidth savings of performing map/reduce at the edge, in
    vehicle networks, by implementing a subset of the Spindle architecture inside a simulation framework.
    We implement the data Spindle processing pipeline for vehicles and Clusterheads, which we then integrate
    with a system that provides realistic mobility and connectivity data, such that we can simulate vehicles
    moving through space, forming clusters, processing map/reduce queries, and producing outputs to send to
    the cloud. We instrument the simulator to measure the number of bytes outputby simulated Clusterheads,
    destined for the cloud. We work in collaboration with Mike Wittie of Montana State University to obtain
    realistic position and connectivity information for vehicles in the simulation. We also work in
    collaboration with Xiaotian Duan, from whose work we get vehicle-to-cluster assignments over time
    for our simulation. In evaluating our experiments, we focus in particular on the effect of different
    reducer window sizes on the amount of data sent to the cloud; in so doing, we explore the trade-off
    between resolution and latency versus bandwidth consumption. Having a smaller reducer window means
    that reduced messages are sent more frequently and client programs are given more data points to
    analyze, while having a larger reducer window size means more messages can be batched together for
    reduction before data is transmitted to the cloud. We also explore the effects of regions with
    differing vehicle densities on bandwidth savings; regions of space that have a higher density
    of vehicles offer the opportunity to form larger clusters and to reduce the total number
    of clusters relative to the total number of vehicles present; a smaller number of clusters
    equates to a smaller number of Clusterheads transmitting data at any given time window, which
    has an effect on overall bandwidth usage.

\chapter{Background and Related Work}
    \section{Connected Vehicles}
        \paragraph{VANETs}
            The past few years have seen increasing interest in the area of "Intelligent Transportation
            Systems (ITS)" in which wireless sensor technologies are put towards the improvement of
            transportation infrastructure. VANETs are wireless ad-hoc networks of vehicles and other
            road infrastructure. Active resarch is underway to find applications for VANETs to improving
            transportation infrastructure and safety. 
            VANETs have unique properties compared to general mesh and ad-hoc networks (MANETs) in that
            the total number of unique nodes that might join a network is quite large and information
            about the velocity and "trajectory" of each node is often available.
            As such, there are a number of low-level VANET protocols being explored including broadcast,
            unicast, and a variety of flooding protocols, however choosing an optimal low-level protocol is 
            beyond the scope of this paper.
            To aid in this research, a number of simulators have
            come out. These simulators combine a mobility model, in which movement of vehicles througuh road
            networks under a variety of driving conditions, and a connectivity model which simulates the
            propagation of radio signals among vehicles in VANETs. OMNeT++ is one such network and connectivity
            simulator.
        \paragraph{Analysis of Vehicle Data with HIVE}
            A 2016 IEEE International Conference on Big Data paper titled "Supporting large scale
            connected vehicle data analysis using HIVE" illustrates practical use cases for analysis
            of vehicle data. The authors use the HIVE \cite{hive} query language in conjunction with Apache Spark
            \cite{spark} and Hadoop MapReduce \cite{hdfs}
            to perform analyses of a 2013 "connected vehicle" dataset from
            United States Department of Transportation. The authors combine the DoT dataset with
            geo-spatial data to validate and improve "traffic demand" models which "can drastically
            improve the planning of incident response strategies." The authors also test travel
            time models for the generation of driving directions and create "traffic flow" visualizations
            to facilitate understanding of road conditions and usage. While the authors demonstrate the 
            usefulness of applying tools such as Apache Spark to data coming from connected vehicles, the
            authors operate in a batched, offline, mode and do not perform edge computing optimizations \cite{cv:hive}.
    \subsection{Vehicle Clusters}
        \paragraph{C-DRIVE}
            C-DRIVE is an approach for forming logical clusters of network-connected vehicles.
            C-DRIVE is specifically designed to take into consideration the properties
            of VANETs with respect to vehicle movement. Specifically C-DRIVE attempts
            to form clusters of networked vehicles by taking into consideration both
            connectivity and direction of travel. These clusters are intended to be
            used to propagate information throughout groups of vehicles in a bandwidth
            -efficient manner \cite{cdrive}.
    \section{Cloud Data Processing Technologies}
        The past few years have seen a number of critical developments in IT
        infrastructure and data processing software that, combined, provide
        a formidable platform for extracting useful information from vast
        quantities of data at scale.
        \paragraph{Amazon Web Services}
            Amazon Web Services (AWS) is a set of "Cloud Computing" offerings
            from Amazon.com that provide access to elastic computation and storage
            resources on an "on-demand" basis \cite{aws}. One particularly valuable
            service offered is EC2, which provides access to collections of virtual
            machines running on Amazon's cloud. These EC2 virtual machines (instances)
            can be used to perform resource-intensive cluster computing operations without
            the need for fixed infrastructure on the part of the end-user. One way in which
            a user can purchase access to EC2 instances is through Spot Requests, wherein the
            user places a bid for the maximum amount s/he is willing to pay per hour for a given
            EC2 instance configuration. If the user is not outbid, then the user is given access
            to an instance of the specified configuration. As soon as the user is outbid (within
            approximately 2 minutes), the user's EC2 instance is shut down and its local data is
            lost. Spot Requests provide very low cost access to computing resources at the cost of
            reliability.%TODO: cite quote how?, TODO: cite docs for spot requests
            Finally of note, Amazon provides an object storage layer - one can associate a key string
            with some binary blob that is stored in a replicated file system - called S3.
    \subsection{Spark Streaming}
        One framework that is capable of processing large quantities of data in a distributed fashion,
        such as on a cluster of EC2 instances, is Apache Spark. Spark offers a micro-batch package for
        processing streaming data called Spark Streaming. Spark Streaming allows a developer (or other
        analyst) to manipulate time-bounded batches of streaming data tuples by writing a program that
        performs immutable transformations such as map, filter, and reduce on a Spark Streaming abstraction
        called a DStream. Spark is able to take the client program, break it up into "stages" in which
        all processing across tuples can occur in parallel, then partition each stream and stage
        across nodes to be processed in parallel. %TODO: clarify how stages work (also double-check)
        In sum, Spark Streaming provides a computing framework and a set of abstractions which allow
        a developer to write map/reduce-style programs for streaming data with relative ease
        \cite{spark:streaming}.
    \subsection{Kafka}
        Apache Kafka is primarily a high-throughput publish-subscribe message bus that runs as a cluster
        of message "brokers" which provide clients access to partitions of logical groups of message streams
        called topics; Kafka brokers support master/slave partition replication and support multiple
        producers and multiple consumers within and across topics and their partitions \cite{kafka}.
        A recent addition to Kafka is Kafka Streams, a streaming data processing framework that
        provides some of the functionality offered by Apache Spark as it pertains to performing basic
        map/reduce-style transformations to streams of tuples. Though Kafka Streams provides per-message
        processing rather than Spark Streaming's micro-batch processing, Kafka Streams provides a similar
        programming interface in which a developer writes map, reduce, filter, and other
        transformations as function applications to data passing through KStreams \cite{kafka:streams}.
    \section{Other Software}
    \subsection{Akka}
        Akka is a JVM library that implements an actor-model system with an Erlang-like structure.
        Akka includes as key features "simple and high-level abstractions for distribution,
        concurrency, and parallelism\dots lightweight event-driven processes\dots[that] can span
        over multiple JVMs;" Akka includes among its use cases "Simulation\dots Automobile and
        Traffic Systems\dots [and] Data Analytics" \cite{akka}. %TODO: ensure this is correct citation for quotes
    \subsection{Postgres}
        Postgres is a relational database with a SQL language interface that supports view, aggregate
        functions, transactions, indices, and joins \cite{postgres}. Postgres is a robust and, when
        properly configured, performant database.
    \section{Edge Computing Technmologies}
    \subsection{Edgent}
        Apache Edgent, formerly Apache Quarks, is a general edge computing and IoT data processing
        platform. Edgent provides connectors for a number of publish/subscribe systems such as 
        MQTT and Apache Kafka as well as methods for performing transformations on streaming
        data on edge devices using pre-programmed analytical subroutines that can be activated and
        deactivated remotely through IBM's "Watson IoT Platform." Edgent appears to be designed
        primarily to run on sensors for monitoring infrastructure and detecting exceptions, though
        it provides a programming model similar to that of Kafka Streams. Edgent appears to be 
        designed along the lines of traditional IoT data collection platforms in which the edge
        device has some set of pre-configured features that can execute locally, while new and
        ad-hoc analytics are run on the "back-end" using a subset of the data the edge device
        was originally programmed to transmit. Edgent does not appear to be designed for executing
        runtime-generated analytic queries in the context of dynamic clusters of vehicles with a diverse
        array of sensor capabilities \cite{edgent}.
    \subsection{Greengrass} %TODO
    \subsection{Cloud IoT Hubs}
        %TODO: aws, bluemix, azure, google cloud, pubnub
%TODO

%TODO (meeting): fix positioing of page numbers

%TODO (meeting): feel free to check in by email to get feedback if necessary

\chapter{System Architecture}
    Spindle provides a novel streaming data processing platform by building on
    existing cutting edge and industry standard distributed systems, primarily
    from the Apache ecosystem. 

    The Spindle system consists of three major components: a set of Apache Spark %TODO: cite
    streaming programs managed by one or more clients, a custom data ingestion and
    query management Middleware, and edge computing software running on network
    connected vehicles.
    
    Spindle operates on the vehicle level as a Scala
    \cite{scala} program running on the Akka \cite{akka} framework; this program
    is responsible for handling cluster formation, data collection, and edge
    computation.

    Data passes through components of the Vehicle software in the form of 
    messages sent to Kafka \cite{kafka} topics. Similarly, data is transferred
    at the cloud layer using Kafka.

    Clients receive data from the middleware as Kafka messages that a custom Spark \cite{spark}
    library processes into DStream tuples which can be further processed using any of the available
    Spark streaming operations written for DStreams and RDDs.

\section{Theorized Architecture}
%GENERAL NOTE: need to be able to understand what the figure is solely through the caption (fine to duplicate text)
    %TODO: walk through spark query, middleware update, propagation through vehicle networks, vehicle-level-map, clusterhead-reduce, transmission to middleware, transmission to spark streaming, secondary reduce, etc..
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{binImages/theoretical-system.png}
        \caption{A diagram of the theorized system's major components. Vehicles
        send data to their Cluster Heads, which perform reduce operations over
        incoming mapped tuples. The resultant reduced tuples are then sent to
        the middleware to be distributed to the connected Spark jobs.}
        \label{fig:theoretical:component}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[height=4in, width=6in]{binImages/theoretical-sequence.png}
        \caption{A sequence diagram showing how a query passes from a connected Spark job
        through the middleware and vehicle systems and how the reduced results then return
        to the Spark job.}
        \label{fig:theoretical:sequence}
    \end{figure}

    %TODO(meeting): do physical

\section{Simulation Architecture}
    \begin{landscape}
        \begin{figure}
            \centering
            \includegraphics[scale=.3]{binImages/simulator-sequence.png}
            %TODO (meeting): increase font size and increase size of figure to be as large as will fit in margin
            \caption{A sequence diagram showing the high-level interactions of
            the Spindle simulator components. A Postgres database stores the
            simulation configurations and pre-computed velocity and connectivity
            data, as well as simulation results.}
        \end{figure}
    \end{landscape}

% Focus more on envisioned 
% Follow Cloud Computing papers for format (upper bound on page count is 5-10 pages)

%separate sections for envisioned architecture and for test design ("accommodations for testing")
    %TODO(meeting): describe mapping of theorized system onto simulator

\chapter{Experiments}
\section{Data Sets}
\subsection{Vehicle Traces}
   The Spindle experiments depend on a set of vehicle traces 
   generated using "the open source vehicular network simulation framework" VEINS, which simulates
   vehicle movement and radio connectivity in a "realistic" manner \cite{veins}. The VEINS traces
   were generated by Mike Wittie of the Montana State University as part of a larger overall research
   project. 
   The traces come in the form of sharded CSV files containing time-series mappings of three different types:
   speed, x-position, y-position, and connectivity to other vehicles. Each time-series mapping includes
   a timestamp, a vehicle uid, and a value (speed, position, reachable-vehicle-id). 
   These shards have been parsed into a Postgres database with a table for each mapping type.
   %TODO(MEETING): insert information about City of Cologne
   % insert information about node count, length of time here
   % Saw we pick sparse, dense region, etc... here (move from subsection below)
\subsection{Test Regions} %TODO(MEETING): make sub-sub section of Vehicle traces subsection
    \paragraph{Original VEINS Traces}
    %TODO (MEETING): insert VEINS visualization here
        The original VEINS traces span a region from $(41735, 85178.4)$ to $(63071.5, 114426)$ and includes 3340 distinct
        vehicles. This dataset includes more vehicles than the current simulator implementation can handle due to the large
        number of threads, network connections, and Kakfa topics that would be required (the thread/topic count scales with the
        number of vehicles multiplied by a constant multiple of the average number of map/reduce queries running on each node).
        A real-world deployment would distribute the threads/topics over the set of vehicles in the system such that each
        vehicle and kafka cluster would have threads and topics proportional to the number of active map/reduce queries.
        In essence, this is a temporary limitation of the simulation architecture, not the real-world architecture.

        To overcome the simulator's limitations, we defined two test regions, one dense region and one sparse region
        being transited by approximately the same number of vehicles. The sparse and dense regions were selected based
        on the number of vehicles transiting them and based on the layout of the underlying road networks.

    \begin{figure}
        \centering
        \includegraphics[scale=.5]{binImages/xiaotian-clusters.png}
        \caption{Illustration of VEINS Vehicles with MANET Clustering, Courtesy Xiaotian Duan}
        \label{fig:clusters}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[scale=.2]{binImages/cars.png}
        \caption{A Snapshot of Active Vehicles in VEINS Traces with Selected Regions Highlighted}
        \label{fig:regions}
    \end{figure}
    \paragraph{Sparse Region}
        The sparse region is transited by 148 distinct vehicles over the course of the simulation and defines a bounding
        box from $(40000, 106800)$ to $(53000, 114000)$. The sparse region covers an area on the outskirts of the city,
        with a relatively low density of vehicles. The sparse region is illustrated by a purple box in \ref{fig:regions}.
    \paragraph{Dense Region}
        The dense region defines a bounding box from $(50000, 100000)$ to $(51000, 102000)$ and is transitied by
        131 vehicles over the course of the simulation. The dense region covers a small area near the city center
        where the density of vehicles is higher. The dense region is illustrated by an orange box in \ref{fig:regions}.

\subsection{Window Sizes}
    Kafka Streams can perform reduce operations over messages received over the course of a user-defined time-window
    and will write the results to a KTable, a logical structure whose API approximates that of a key-value store. %TODO: citations
    Spindle's Kafka Streams reducer adds a \verb|Batcher|, which transmits the final value of a time-window's KTable
    entry to some destination Kafka topic (in this case the middleware). As such, Spindle supports micro-batch operations
    on Kafka streams with user-configurable window sizes. The sizes tested were 10, 15, and 30 seconds.
\subsection{Test Clusters}
    The cluster head assignments for each node are pre-computed using either Xiaotian's %TODO: cite Xiaotian
    clustering algorithm or some base-line \verb|single_clusterhead| (all vehicles in simulation share a single clusterhead)
    or \verb|self_clusters| (all vehicles act as their own cluster heads).
\subsection{Clustering Algorithms}
    The spindle architecture takes advantage of work done by fellow RPI student Xiaotian Duan on generating
    clusters of vehicles based on vehicle connectivity and lane position. 
    %TODO: get permission from Xiaotian for using clustering images
    %TODO: include information about cluster density for each clustering (sparse/dense)
    %TODO: Fix the quotes!

\section{Test Software}
    \paragraph{SpindleSim}
        The Spindle simulator (SpindleSim) is implemented in several thousand lines of Scala %TODO: cite
        and is implemented on top of the Akka %TODO: cite
        actor model framework. The Scala/Akka portions of the simulator implement as realistically
        as is practical the software that would run on a real-world deployment of Spindle. This is
        accomplished by creating a separate Akka actor for each simulated vehicle, where each actor
        is given a cache of time-series simulation events and data and a connection to a ``World''
        actor responsible for keeping track of global simulation time; this global time-keeper 
        can be thought of as a stand-in for a GPS clock running on each vehicle. Each vehicle
        actor is then responsible for managing its publish-subscribe streams and messages, as well
        as its simulated connections to other vehicles in its cluster. Messages are exchanged between
        vehicles and send from vehicles to ``the cloud" by way of special "Stream Relay" Kakfa Streams
        programs responsible for filtering expired and "Canary" messages %TODO: explain redundant messages in architecture
        and responsible for keeping a running sum of the number of bytes sent on a per-relay basis and logging
        these sums to separate CSV files at run-time.
    \paragraph{AutoSim}
        The Scala/Akka simulator (SpindleSim) currently has a number of pre-configured map/reduce jobs written
        that can be turned on or off using the program's \verb|application.conf| file. A secondary
        program, dubbed AutoSim, runs the simulator iteratively, writing to the conf file, launching
        SpindleSim, then gathering the results and uploading them to a database and permanent storage.
        In order to iterate through the desired test configurations, AutoSim reads from a table \verb|sim_configs_vx| in a 
        Postgres 9.6.1 database hosted on an AWS EC2 \verb|m4.large| reserved instance.
        AutoSim will choose among the least-tested configurations in the configs table, write the configuration to
        the SpindleSim \verb|application.conf|, and launch an instance of SpindleSim. While SpindleSim is running, AutoSim
        greps the console logs for exception messages. If an exception is detected, the current SpindleSim instance is killed
        and a notification is sent via AWS SNS topic reporting the error message in order to facilitate debugging. If the SpindleSim
        operation completes, AutoSim uploads the CSV logs to an AWS S3 bucket and parses the final message byte sums from the CSV
        files then writes the results to a \verb|sim_results_vx| table in Postgres. AutoSim is a simple Node.JS ES6 application
        that is run from the Babel-Node %TODO: CITE
        transpiler.
    \paragraph{Environment}
        AutoSim and SpindleSim are packaged inside a docker image, \\\verb|wkronmiller/nsl-spindle-simulator| that can be run from
        a laptop, desktop, or EC2 instance. The test framework and container are designed to survive the total loss of local
        storage and/or the running simulator container by storing the results of all completed simiulation operations on
        S3 and a remote database. This design decision allows the test framework to be run on an AWS EC2 Spot Fleet %TODO: citation
        which offers discounts of roughly 70-90\% in exchange for the requirement that any software running on a Spot instance
        be designed to killed at any time with little or no warning. %TODO citation
        The architecture also enables trivial transfer of the simulator across different machines, easing debugging and mitigating
        problems related to transient EC2 network problems. 
\section{Test Configurations}
\subsection{Example Map/Reduce Programs}
    \paragraph{speedSum}
        The most simple map/reduce operation tested is the "speedSum" job, which simply maps each vehicle's available data to a
        tuple containing only the vehicle's speed. The reduce operation run at the clusterheads takes the sum of the speeds
        of each of the cluster member vehicles.
%TODO (MEETING): each paragraph should be formatted as: use case, query format
        % first sentence - query computes x
        % follow-on: uses this map function, uses reduce function
    \paragraph{geoFiltered}
        This map/reduce job extends speedSum by performing a word-count-style map operation that maps each vehicle's sensors
        to the vehicle count and the vehicle speed: \verb|(1, [speed])|. The reduce operation again simply sums the count
        and speed of each member vehicle \verb|([numVehiclesInCluster], [sumOfSpeeds])|. The geoFiltered query also is selective,
        in that for a given test region the geoFiltered query operates only on a subset of the test region containing approximately
        half of the vehicles being tested.
    \paragraph{geoMapped}
        The geoMapped query performs a similar map/reduce operation to geoFiltered, but instead of filtering out half the
        test region, the geoMapped query maps half the test region to one region ID and maps the other half of the
        test region to a second region ID, then performs a reducebykey. The map operation produces the following:
        \verb|(regionId) -> (1, [speed])| and the reduce produces \verb|(regionId) -> ([numVehiclesInCluster], [sumOfSpeeds])|.

\section{Simulator Results}
    \begin{figure}
        \centering
        \includegraphics[scale=.8]{binImages/speedSum-runplot.pdf}
        %NOTE (Meeting)
        \caption{The total number of bytes sent to the middleware,
        averaged over multiple trials for the speedSum map/reduce job. The x-axis contians different
        configurations of regions, clusterings, and reduce window sizes (10, 15, and 30 seconds). Smaller
        values in the same geographic region indicate better performance, where the best performance
        comes from using clustering with a 30 second window size.}
    \end{figure}
    %TODO (meeting): need number of trials
    %TODO (meeting): put related figures (normed not normed) on same page, try using subfig package 
    \begin{figure}
        \centering
        \includegraphics[scale=.8]{binImages/speedSum-runplot-normalized.pdf}
        \caption{shows the scaled number of bytes sent to the middleware in a given region and window size
        configuration where y=1 indicates the number of bytes sent without clustering for a given region
        and reduce window size. The figure demonstrates the data savings that occur as a result of using
        vehicle clusters. The figure also illustrates how data reduction is affected by regional vehicle
        density - areas of high vehicle density can take better advantage of clustering to get more savings.}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[scale=.8]{binImages/geoMapped-runplot.pdf}
        \caption{shows the total number of bytes sent to the middleware,
        averaged over multiple trials for the geoMapped map/reduce job. The x-axis contians different
        configurations of regions, clusterings, and reduce window sizes (10, 15, and 30 seconds).
        Despite being a slightly different map/reduce query, the performance of the geoMapped tests
        is similar to that of the speedSum tests, where the best performing configuration is the clustered
        configuration with a 30-second reduce window size.}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[scale=.8]{binImages/geoMapped-runplot-normalized.pdf}
        \caption{shows the scaled number of bytes sent to the middleware in a given region and window size
        configuration where y=1 indicates the number of bytes sent without clustering for a given region
        and reduce window size. The figure shows a similar reduction in data transfer by using clusters
        to that of the speedSum map/reduce job.}
    \end{figure}

    %TODO: explain results in words
\chapter{Conclusion}
%TODO: what did we do/show?
\section{Future Work}
%TODO

%\chapter{THE NEXT CHAPTER}
%And so on, for more chapters.
%Another citation for the bibliography:\cite{anotherbook}

% The following produces a numbered bibliography where the numbers
% correspond to the \cite commands in the text.
\specialhead{LITERATURE CITED}
\begin{singlespace}
\begin{thebibliography}{99}
\bibitem{veins}
    http://veins.car2x.org %TODO: proper citation format (Feb 22, 17)
\bibitem{scala}
    http://www.scala-lang.org %TODO: proper citation format Feb 26, 17
\bibitem{akka}
    http://akka.io % Feb 26
\bibitem{kafka}
    https://kafka.apache.org % Feb 26
        %TODO: cite papers
\bibitem{spark}
    http://spark.apache.org/streaming/ %Feb 26
        %TODO: cite spark and spark streaming papers, not the URL
\bibitem{hitachi}
    https://www.hds.com/en-ous/pdf/white-paper/hitachi-white-paper-internet-on-wheels.pdf
\bibitem{mapreduce}
    https://research.google.com/archive/mapreduce.html
\bibitem{aws}
    http://docs.aws.amazon.com/gettingstarted/latest/awsgsg-intro/awsgsg-intro.pdf %March 1
\bibitem{spark:streaming}
    http://people.csail.mit.edu/matei/papers/2013/sosp\_spark\_streaming.pdf %March 1
\bibitem{kafka:streams}
    http://docs.confluent.io/3.0.0/streams/
\bibitem{akka}
    http://doc.akka.io/docs/akka/2.4/AkkaScala.pdf
\bibitem{postgres}
    https://www.postgresql.org/files/documentation/pdf/9.6/postgresql-9.6-US.pdf
\bibitem{kafka}
@article{Wang:2015:BRL:2824032.2824063,
     author = {Wang, Guozhang and Koshy, Joel and Subramanian, Sriram and Paramasivam, Kartik and Zadeh, Mammad and Narkhede, Neha and Rao, Jun and Kreps, Jay and Stein, Joe},
     title = {Building a Replicated Logging System with Apache Kafka},
     journal = {Proc. VLDB Endow.},
     issue\_date = {August 2015},
     volume = {8},
     number = {12},
     month = aug,
     year = {2015},
     issn = {2150-8097},
     pages = {1654--1655},
     numpages = {2},
     url = {http://dx.doi.org/10.14778/2824032.2824063},
     doi = {10.14778/2824032.2824063},
     acmid = {2824063},
     publisher = {VLDB Endowment},
    } 
\bibitem{connectivitypaper}
@ARTICLE{6823640, 
    author={N. Lu and N. Cheng and N. Zhang and X. Shen and J. W. Mark}, 
    journal={IEEE Internet of Things Journal}, 
    title={Connected Vehicles: Solutions and Challenges}, 
    year={2014}, 
    volume={1}, 
    number={4}, 
    pages={289-299}, 
    keywords={intelligent transportation systems;next generation networks;road vehicles;vehicular ad hoc networks;Internet of Vehicles;IoV;automotive revolution;connected vehicles;next generation intelligent transportation systems;vehicle communication;vehicle-to-Internet infrastructure connectivity;vehicle-to-road infrastructure connectivity;vehicle-to-sensor infrastructure connectivity;vehicle-to-vehicle infrastructure connectivity;vehicle-to-x connectivity;wireless
    connectivities;Communication system security;Reliability;Safety;Vehicles;Wireless communication;Wireless sensor networks;Zigbee;Connected vehicles;Internet of Vehicles (IoV);intelligent transportation systems (ITSs);intra-vehicle wireless sensor networks;vehicular networks}, 
    doi={10.1109/JIOT.2014.2327587}, 
    ISSN={2327-4662}, 
    month={Aug},}
\bibitem{cdrive}
@INPROCEEDINGS{5720653, 
    author={N. Maslekar and M. Boussedjra and J. Mouzna and H. Labiod}, 
    booktitle={2011 4th IFIP International Conference on New Technologies, Mobility and Security}, 
    title={C-DRIVE: Clustering Based on Direction in Vehicular Environment}, 
    year={2011}, 
    pages={1-5}, 
    keywords={pattern clustering;traffic engineering computing;vehicular ad hoc networks;C-DRIVE;VANET;adaptive traffic signal control;data dissemination;direction based clustering algorithm;vehicle density estimation;vehicular environment;Algorithm design and analysis;Bandwidth;Clustering algorithms;IP networks;Mobile communication;Protocols;Vehicles}, 
    doi={10.1109/NTMS.2011.5720653}, 
    ISSN={2157-4952}, 
    month={Feb},}
\bibitem{cv:hive}
@INPROCEEDINGS{7840862, 
    author={W. Xu and N. R. Juri and A. Gupta and A. Deering and C. Bhat and J. Kuhr and J. Archer}, 
    booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
    title={Supporting large scale connected vehicle data analysis using HIVE}, 
    year={2016}, 
    pages={2296-2304}, 
    keywords={data analysis;data handling;road vehicles;traffic engineering computing;vehicular ad hoc networks;CV data analysis;HIVE;MapReduce programming framework;Spark programming framework;connected vehicle data analysis;data analytic tasks;data storage;roadside infrastructure;safety-related information;transportation operations;transportation planning research;vehicle connectivity;vehicle message exchange;Data analysis;Planning;Programming;Safety;Sparks;Vehicles;Big Data;Connected Vehicles Data;Hive;Spark;Transportation Planning}, 
    doi={10.1109/BigData.2016.7840862}, 
    month={Dec},}
\bibitem{edgent}
    https://edgent.apache.org
\bibitem{hive}
@INPROCEEDINGS{5447738, 
author={A. Thusoo and J. S. Sarma and N. Jain and Z. Shao and P. Chakka and N. Zhang and S. Antony and H. Liu and R. Murthy}, 
booktitle={2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)}, 
title={Hive - a petabyte scale data warehouse using Hadoop}, 
year={2010}, 
pages={996-1005}, 
keywords={SQL;competitive intelligence;data warehouses;public domain software;query processing;Hadoop software;HiveQL language;Metastore system catalog;SQL-like declarative language;arrays;business intelligence;data exploration;map-reduce jobs;maps;nested compositions;open-source map-reduce implementation;petabyte scale data warehouse;primitive types;query compilation;query optimization;Companies;Data warehouses;Facebook;Hardware;Libraries;Open source software;Plugs;Query processing;Statistics;Warehousing}, 
doi={10.1109/ICDE.2010.5447738}, 
ISSN={1063-6382}, 
month={March},}
\bibitem{hdfs}
@inproceedings{Shvachko:2010:HDF:1913798.1914427,
 author = {Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
 title = {The Hadoop Distributed File System},
 booktitle = {Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
 series = {MSST '10},
 year = {2010},
 isbn = {978-1-4244-7152-2},
 pages = {1--10},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/MSST.2010.5496972},
 doi = {10.1109/MSST.2010.5496972},
 acmid = {1914427},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 
\bibitem{vanet:simulators}
@INPROCEEDINGS{6103008, 
author={E. Spaho and L. Barolli and G. Mino and F. Xhafa and V. Kolici}, 
booktitle={2011 International Conference on Broadband and Wireless Computing, Communication and Applications}, 
title={VANET Simulators: A Survey on Mobility and Routing Protocols}, 
year={2011}, 
pages={1-10}, 
keywords={routing protocols;telecommunication computing;vehicular ad hoc networks;VANET simulator;general purpose distributed wireless networks;inter vehicle communication;mobility model;mobility survey;network simulator;routing protocol;vehicular ad hoc networks;vehicular traffic simulator;Ad hoc networks;Object oriented modeling;Roads;Routing;Routing protocols;Vehicles;Mobility Models;Routing Protocols;Simulators;VANETs}, 
doi={10.1109/BWCCA.2011.11}, 
month={Oct},}
%TODO: figure out bibtex
    
\end{thebibliography}
\end{singlespace}
\end{document}
